import streamlit as st
from langgraph.graph import StateGraph, END
from langchain_openai import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI as LCChatOpenAI
from langchain_community.tools.wikipedia.tool import WikipediaQueryRun
from langchain_community.utilities.wikipedia import WikipediaAPIWrapper
from langchain_chroma import Chroma
from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor
from langchain.prompts import ChatPromptTemplate as LCChatPromptTemplate
from textwrap import dedent
from typing import TypedDict
from dotenv import load_dotenv
import os
import re

# âœ… í™˜ê²½ ì„¤ì •
env_path = "C:/Aicamp/SKN13_my/13_Langchain/.env"
load_dotenv(dotenv_path=env_path)

COSINE_SIMILARITY_THRESHOLD = 0.1

CHROMA_DBS_CONFIG = [
    {"persist_directory": "database/vector_store/chroma", "collection_name": "antibiotic_overuse"},
    {"persist_directory": "database/vector_store/chroma", "collection_name": "vitamin"},
    {"persist_directory": "database/vector_store/others", "collection_name": "others"},
    {"persist_directory": "database/vector_store/vitamin_chroma", "collection_name": "vitamin"},
    {"persist_directory": "database/vector_store/soomin", "collection_name": "soomin"},
    {"persist_directory": "database/vector_store/chroma_pubmed_v3_large", "collection_name": "langchain"},
]

class CompareState(TypedDict):
    question: str
    translated_question: str
    social_knowledge: str
    latest_docs: list
    k_value: int
    final_answer: str
    db_has_data: bool

model = LCChatOpenAI(model="gpt-4.1", temperature=0)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")

COMPARE_PROMPT = LCChatPromptTemplate.from_messages([
    ("system", dedent("""
    ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ë£Œ ì •ë³´ ìš”ì•½ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸, ì‚¬íšŒí†µë… ì •ë³´, ìµœì‹  ì˜í•™ ë…¼ë¬¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ë‹µë³€ë§Œ ê°„ê²°í•˜ê³  ëª…í™•í•˜ê²Œ ì‘ì„±í•˜ì„¸ìš”.
    - ìµœì‹  ì—°êµ¬ ê²°ê³¼ê°€ ê¸°ì¡´ ì‚¬íšŒí†µë…ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµí•˜ê³  ì¢…í•©ì ì¸ í•œê¸€ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
    - ì‚¬íšŒí†µë…ì´ë‚˜ ìµœì‹  ë…¼ë¬¸ ì •ë³´ê°€ ì—†ë‹¤ë©´ í•´ë‹¹ ë¶€ë¶„ì€ 'ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ì‘ì„±í•˜ì„¸ìš”.
    - ìµœì‹  ì˜í•™ ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì— ê´€ë ¨ ì •ë³´ê°€ ì—†ëŠ” ê²½ìš°, ê·¸ ì‚¬ì‹¤ì„ ëª…í™•íˆ ì–¸ê¸‰í•´ì£¼ì„¸ìš”.
    ë‹µë³€ì€ ì•„ë˜ì™€ ê°™ì´ 3ë¬¸ë‹¨ìœ¼ë¡œ êµ¬ì„±í•´ì„œ í•œê¸€ë¡œ ìµœì¢… ë‹µë³€í•´ì£¼ì„¸ìš”.
    1. ì‚¬íšŒí†µë… ì†Œê°œ
    2. ìµœì‹  ë…¼ë¬¸ ì†Œê°œ
    3. ê°„ë‹¨í•œ ë¹„êµê¸€
    """)),
    ("human", dedent("""
    ì‚¬ìš©ì ì§ˆë¬¸: {question}
    [ì‚¬íšŒí†µë… ì •ë³´]
    {social_knowledge}
    [ìµœì‹  ë…¼ë¬¸ ì •ë³´]
    {latest_docs}
    {db_status_message}
    ìœ„ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ìƒì„¸í•˜ê³  ì‰½ê²Œ ì´í•´í•  ìˆ˜ ìˆëŠ” í•œê¸€ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
    """))
])

@st.cache_resource
def get_chroma_instance(collection_name, persist_dir):
    return Chroma(
        collection_name=collection_name,
        persist_directory=persist_dir,
        embedding_function=embedding_model
    )

def translate_question(state: CompareState) -> CompareState:
    translate_prompt = f"Translate the following Korean medical question into English: {state['question']}"
    translated = model.invoke(translate_prompt).content
    state["translated_question"] = translated
    return state

def agent_search(state: CompareState) -> CompareState:
    wiki_wrapper = WikipediaAPIWrapper(top_k_results=3)
    wiki_tool = WikipediaQueryRun(api_wrapper=wiki_wrapper)

    tools = [Tool(name="Wikipedia", func=wiki_tool.run, description="ì‚¬íšŒí†µë… ê²€ìƒ‰")]

    AGENT_PROMPT = LCChatPromptTemplate.from_messages([
        ("system", dedent("""
        ë„ˆëŠ” ë˜‘ë˜‘í•œ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì‚¬íšŒí†µë… ì •ë³´ë¥¼ Wikipediaì—ì„œ ê²€ìƒ‰í•´ì•¼ í•´.
        ê²€ìƒ‰ëœ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ **ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨ëœ í•µì‹¬ ì‚¬íšŒí†µë…**ì„ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì„œ ë°˜í™˜í•´ì•¼ í•´.
        ë§Œì•½ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ë‹¤ë©´, 'ê´€ë ¨ ì‚¬íšŒí†µë… ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ì•Œë ¤ì¤˜.
        """)),
        ("human", "{input}\n\n{agent_scratchpad}")
    ])

    agent = create_openai_functions_agent(model, tools, prompt=AGENT_PROMPT)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=False)

    search_query = f"Search for common knowledge about: {state['translated_question']}"
    result = executor.invoke({"input": search_query})

    if result and result.get("output"):
        content = result["output"].strip()
        if content.lower() in ["ê´€ë ¨ ì‚¬íšŒí†µë… ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ", "no information found"]:
            state["social_knowledge"] = "ì •ë³´ ì—†ìŒ"
        else:
            state["social_knowledge"] = content
    else:
        state["social_knowledge"] = "ì •ë³´ ì—†ìŒ"

    return state

def get_latest_papers(state: CompareState) -> CompareState:
    all_docs = []
    found_any = False

    for db_config in CHROMA_DBS_CONFIG:
        db = get_chroma_instance(db_config["collection_name"], db_config["persist_directory"])
        try:
            results = db.similarity_search_with_score(state["translated_question"], k=state["k_value"])
            for doc, score in results:
                if score < (1 - COSINE_SIMILARITY_THRESHOLD):
                    all_docs.append(doc.page_content)
                    found_any = True
        except Exception as e:
            print(f"âŒ DB ì˜¤ë¥˜: {e}")

    state["latest_docs"] = all_docs
    state["db_has_data"] = found_any
    return state

def compare_and_summarize(state: CompareState) -> CompareState:
    social_content = state["social_knowledge"] if state["social_knowledge"] else "ì •ë³´ ì—†ìŒ"
    latest_content = "\n\n".join(state["latest_docs"]) if state["latest_docs"] else "ì •ë³´ ì—†ìŒ"

    db_status_message = "" if state["db_has_data"] else "ì°¸ê³ : ìµœì‹  ì˜í•™ ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì— í•´ë‹¹ ì •ë³´ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤."

    prompt_value = COMPARE_PROMPT.format(
        question=state["question"],
        social_knowledge=social_content,
        latest_docs=latest_content,
        db_status_message=db_status_message
    )
    response = model.invoke(prompt_value)
    final = response.content

    # ë¬¸ë‹¨ êµ¬ë¶„ì„ ìœ„í•´ ë²ˆí˜¸ ê¸°ì¤€ìœ¼ë¡œ ê°œí–‰ ì²˜ë¦¬
    final = re.sub(r"2\.", "\n\n2.", final)
    final = re.sub(r"3\.", "\n\n3.", final)

    state["final_answer"] = final
    return state

def build_compare_graph():
    workflow = StateGraph(CompareState)
    workflow.add_node("translate_question", translate_question)
    workflow.add_node("agent_search", agent_search)
    workflow.add_node("get_latest_papers", get_latest_papers)
    workflow.add_node("compare_and_summarize", compare_and_summarize)

    workflow.set_entry_point("translate_question")
    workflow.add_edge("translate_question", "agent_search")
    workflow.add_edge("agent_search", "get_latest_papers")
    workflow.add_edge("get_latest_papers", "compare_and_summarize")
    workflow.add_edge("compare_and_summarize", END)

    return workflow.compile()

# --- Streamlit ì•± UI (ì±„íŒ… íˆìŠ¤í† ë¦¬) ---
st.title("ğŸ’¬ ìµœì‹  ë…¼ë¬¸ ê¸°ë°˜ ì˜ë£Œ ì±—ë´‡")

if "messages" not in st.session_state:
    st.session_state["messages"] = []

user_input = st.chat_input("ê¶ê¸ˆí•œ ì ì„ ì…ë ¥í•˜ì„¸ìš”!")

if user_input:
    # ì´ì „ ëŒ€í™” ì €ì¥
    st.session_state["messages"].append({"role": "user", "content": user_input})

    graph = build_compare_graph()
    state = {
        "question": user_input,
        "translated_question": "",
        "social_knowledge": "",
        "latest_docs": [],
        "k_value": 5,
        "final_answer": "",
        "db_has_data": False
    }
    result = graph.invoke(state)

    # ë‹µë³€ ì €ì¥
    st.session_state["messages"].append({"role": "assistant", "content": result["final_answer"]})

# íˆìŠ¤í† ë¦¬ ì¶œë ¥ (ê³¼ê±°ëŠ” ìœ„, ìµœì‹ ì€ ì•„ë˜)
for msg in st.session_state["messages"]:
    if msg["role"] == "user":
        with st.chat_message("user"):
            st.write(msg["content"])
    else:
        with st.chat_message("assistant"):
            st.markdown(msg["content"], unsafe_allow_html=True)
