import streamlit as st
from langchain_openai import OpenAIEmbeddings
from langchain_community.chat_models import ChatOpenAI as LCChatOpenAI
from langchain_chroma import Chroma
from langgraph.graph import StateGraph, END
from langchain.prompts import ChatPromptTemplate as LCChatPromptTemplate
from langchain_community.utilities.wikipedia import WikipediaAPIWrapper
from langchain_community.tools.wikipedia.tool import WikipediaQueryRun
from langchain.agents import create_openai_functions_agent, Tool, AgentExecutor
from textwrap import dedent
from typing import TypedDict
import os
from dotenv import load_dotenv

# âœ… í™˜ê²½ ì„¤ì •
env_path = "C:/Aicamp/SKN13_my/13_Langchain/.env"
load_dotenv(dotenv_path=env_path)

# âœ… ëª¨ë¸ ë° ì„¤ì •
model = LCChatOpenAI(model="gpt-4o", temperature=0, streaming=True)
embedding_model = OpenAIEmbeddings(model="text-embedding-3-large")
COSINE_SIMILARITY_THRESHOLD = 0.1

CHROMA_DBS_CONFIG = [
    {"persist_directory": "Europe_PMC/vector_store/chroma", "collection_name": "antibiotic_overuse"},
    {"persist_directory": "Europe_PMC/vector_store/others", "collection_name": "others"},
    {"persist_directory": "Europe_PMC/vector_store/vitamin_chroma", "collection_name": "vitamin"}
]

class CompareState(TypedDict):
    question: str
    translated_question: str
    social_knowledge: str
    latest_docs: list
    k_value: int
    final_answer: str
    db_has_data: bool

COMPARE_PROMPT = LCChatPromptTemplate.from_messages([
    ("system", dedent("""
    ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ë£Œ ì •ë³´ ìš”ì•½ê°€ì…ë‹ˆë‹¤.
    ì•„ë˜ ì‚¬ìš©ì ì§ˆë¬¸, ì‚¬íšŒí†µë… ì •ë³´, ìµœì‹  ì˜í•™ ë…¼ë¬¸ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ,
    ìµœì‹  ì—°êµ¬ ê²°ê³¼ê°€ ê¸°ì¡´ ì‚¬íšŒí†µë…ê³¼ ì–´ë–»ê²Œ ë‹¤ë¥¸ì§€ ë¹„êµí•˜ê³  ì¢…í•©ì ì¸ í•œê¸€ ìš”ì•½ì„ ì‘ì„±í•˜ì„¸ìš”.
    """)),
    ("human", dedent("""
    ì‚¬ìš©ì ì§ˆë¬¸: {question}

    [ì‚¬íšŒí†µë… ì •ë³´]
    {social_knowledge}

    [ìµœì‹  ë…¼ë¬¸ ì •ë³´]
    {latest_docs}

    {db_status_message}
    """))
])

# âœ… ë…¸ë“œ ì •ì˜
def translate_question(state: CompareState) -> CompareState:
    translate_prompt = f"Translate the following Korean medical question into English: {state['question']}"
    translated = model.invoke(translate_prompt).content
    state["translated_question"] = translated
    return state

def agent_search(state: CompareState) -> CompareState:
    wiki_wrapper = WikipediaAPIWrapper(top_k_results=3)
    wiki_tool = WikipediaQueryRun(api_wrapper=wiki_wrapper)
    tools = [Tool(name="Wikipedia", func=wiki_tool.run, description="ì‚¬íšŒí†µë… ê²€ìƒ‰")]
    AGENT_PROMPT = LCChatPromptTemplate.from_messages([
        ("system", dedent("""
        ë„ˆëŠ” ë˜‘ë˜‘í•œ ì „ë¬¸ê°€ì•¼. ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ì‚¬íšŒí†µë… ì •ë³´ë¥¼ Wikipediaì—ì„œ ê²€ìƒ‰í•˜ê³ ,
        ê´€ë ¨ ì •ë³´ë¥¼ í•œêµ­ì–´ë¡œ ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì„œ ë°˜í™˜í•´ì•¼ í•´.
        ì—†ìœ¼ë©´ 'ê´€ë ¨ ì‚¬íšŒí†µë… ì •ë³´ ì—†ìŒ'ì´ë¼ê³  ì•Œë ¤ì¤˜.
        """)),
        ("human", "{input}\n\n{agent_scratchpad}")
    ])
    agent = create_openai_functions_agent(model, tools, prompt=AGENT_PROMPT)
    executor = AgentExecutor(agent=agent, tools=tools, verbose=False)

    search_query = f"Search for common knowledge about: {state['translated_question']}"
    try:
        result = executor.invoke({"input": search_query})
        if result and result.get("output"):
            content = result["output"].strip()
            if content.lower() in ["ê´€ë ¨ ì‚¬íšŒí†µë… ì •ë³´ ì—†ìŒ", "ì •ë³´ ì—†ìŒ"]:
                state["social_knowledge"] = "ì •ë³´ ì—†ìŒ"
            else:
                state["social_knowledge"] = content
        else:
            state["social_knowledge"] = "ì •ë³´ ì—†ìŒ"
    except:
        state["social_knowledge"] = "ì •ë³´ ì—†ìŒ"
    return state

def get_latest_papers(state: CompareState) -> CompareState:
    all_docs = []
    found_data = False
    for db_config in CHROMA_DBS_CONFIG:
        try:
            db = Chroma(
                collection_name=db_config["collection_name"],
                persist_directory=db_config["persist_directory"],
                embedding_function=embedding_model
            )
            results = db.similarity_search_with_score(state["translated_question"], k=state["k_value"])
            for doc, score in results:
                if score < (1 - COSINE_SIMILARITY_THRESHOLD):
                    all_docs.append(doc.page_content)
                    found_data = True
        except:
            continue
    state["latest_docs"] = all_docs
    state["db_has_data"] = found_data
    return state

def compare_and_summarize(state: CompareState) -> CompareState:
    social_content = state["social_knowledge"] or "ì •ë³´ ì—†ìŒ"
    latest_content = "\n\n".join(state["latest_docs"]) or "ì •ë³´ ì—†ìŒ"
    db_status_msg = "" if state["db_has_data"] else "ì°¸ê³ : ìµœì‹  ì˜í•™ ë…¼ë¬¸ ë°ì´í„°ë² ì´ìŠ¤ì— ì •ë³´ê°€ ë¶€ì¡±í•˜ê±°ë‚˜ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    prompt_value = COMPARE_PROMPT.format(
        question=state["question"],
        social_knowledge=social_content,
        latest_docs=latest_content,
        db_status_message=db_status_msg
    )
    chunks = model.stream(prompt_value)
    final_content = ""
    placeholder = st.empty()
    for chunk in chunks:
        final_content += chunk.content
        placeholder.markdown(final_content + "â–Œ")
    placeholder.markdown(final_content)
    state["final_answer"] = final_content
    return state

def build_compare_graph():
    workflow = StateGraph(CompareState)
    workflow.add_node("translate_question", translate_question)
    workflow.add_node("agent_search", agent_search)
    workflow.add_node("get_latest_papers", get_latest_papers)
    workflow.add_node("compare_and_summarize", compare_and_summarize)

    workflow.set_entry_point("translate_question")
    workflow.add_edge("translate_question", "agent_search")
    workflow.add_edge("agent_search", "get_latest_papers")
    workflow.add_edge("get_latest_papers", "compare_and_summarize")
    workflow.add_edge("compare_and_summarize", END)

    return workflow.compile()

# ================================
# Streamlit ì•±
# ================================
st.title("ğŸ©º ì˜í•™ ë…¼ë¬¸ ë¹„êµ ìš”ì•½ ì±—ë´‡")

if "history" not in st.session_state:
    st.session_state.history = []

# ê³¼ê±° ëŒ€í™” ë¨¼ì € ì¶œë ¥
for msg in st.session_state.history:
    if msg["type"] == "human":
        with st.chat_message("user"):
            st.markdown(msg["content"])
    else:
        with st.chat_message("assistant"):
            st.markdown(msg["content"])

user_input = st.chat_input("ê¶ê¸ˆí•œ ì˜í•™ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”!")

if user_input:
    # ì§ˆë¬¸ ë¨¼ì € í‘œì‹œ (ì¶”ê°€ X)
    with st.chat_message("user"):
        st.markdown(user_input)

    graph = build_compare_graph()

    initial_state = {
        "question": user_input,
        "translated_question": "",
        "social_knowledge": "",
        "latest_docs": [],
        "k_value": 5,
        "final_answer": "",
        "db_has_data": False
    }

    # ì‘ë‹µ í‘œì‹œ ë° streaming
    with st.chat_message("assistant"):
        result = graph.invoke(initial_state)
        final_answer = result["final_answer"]

    # ëŒ€í™” historyì— ì§ˆë¬¸ê³¼ ì‘ë‹µì„ í•œ ë²ˆì— ì¶”ê°€
    st.session_state.history.append({"type": "human", "content": user_input})
    st.session_state.history.append({"type": "assistant", "content": final_answer})
